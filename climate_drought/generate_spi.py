import logging
import os
from os.path import expanduser
import shutil
import sys
import numpy as np
import subprocess
from pathlib import Path
from typing import List
from enum import Enum, unique, auto
from datetime import date, time, datetime
# handling NetCDF files
import xarray
import pandas as pd
import json
from climate_drought import indices, utils
import matplotlib.pyplot as plt
# pygeometa for OGC API record creation
import yaml
import ast
import re
from pygeometa.core import read_mcf
from pygeometa.schemas.ogcapi_dataset_records import OGCAPIDRecordOutputSchema

# Logging
logging.basicConfig(level=logging.DEBUG)

# ERA5 download range
Sdate = '19850101'
Edate = '20221231'


class Era5ProcessingBase:
    """
    Provides some basic functionality that can be used by different implementation specific strategies for different
    data sources
    """
    # Code to downloading data from the Copernicus Climate Data Store.
    # If the 'pixutils' module is installed this script should be located in the 'bin' directory of the Conda environment else setup link to pixutils

    era_download = "era_download.py"
    home = expanduser("~")
    python_env = os.path.join(home, "anaconda3/envs/climate_env/bin")
    ERA_DOWNLOAD_PY = os.path.join(python_env, era_download)
    if not os.path.exists(ERA_DOWNLOAD_PY):
        ERA_DOWNLOAD_PY = os.path.join("pixutils", era_download)

    if not os.path.exists(ERA_DOWNLOAD_PY):
        print("could not find {}, exiting".format(era_download))
        sys.exit(1)

    # Need to call from python env is not in bin folder
    if "pixutils" in ERA_DOWNLOAD_PY:
        ERA_DOWNLOAD_PY = r'{} {}'.format(os.path.join(python_env,"python"),ERA_DOWNLOAD_PY)


    #   target download time for each data source
    SAMPLE_TIME = time(hour=12, minute=0)

    def __init__(self, args, working_dir: str):
        """
        Initializer; should be called by derived classes
        :param args: program arguments
        :param working_dir: directory that will hold all files generated by the class
        """
        self.logger = logging.getLogger("ERA5_Processing")
        self.logger.setLevel(logging.DEBUG) if args.verbose else self.logger.setLevel(logging.INFO)
        self.args = args

        # Create list of dates between max start and end dates
        dates = utils.daterange(Sdate, Edate, 0)
        date_list = []
        for i,indate in enumerate(dates):
            self.args.year = int(indate[0:4])
            self.args.month = int(indate[4:6])
            self.args.day = int(indate[6:8])
            date_list.append(date(self.args.year, self.args.month, self.args.day))
        self.args.dates = date_list
        self.working_dir = working_dir

    @property
    def file_str(self) -> str:
        """
        Utility function to convert date and location to a defined string format
        :return: a date string formatted as "YYYY-MM" or "YYYY-MM-DD", depending on if a day is specified, and latitude/longitude coordinates
        """
        return "{sd}-{ed}_{la}_{lo}".format(sd=self.args.start_date, ed=self.args.end_date, la=self.args.latitude, lo=self.args.longitude)

    def download(self) -> str:
        """
        This function handles downloading from the Copernicus Climate Service.
        Functionality should be suitable for most uses, but can be overridden by derived classes for more specialised
        downloads
        :return: path to the file containing the downloaded data
        """
        self.logger.info("Initiating download of ERA5 data.")
        self.logger.info("Variables to be downloaded: {}.".format(", ".join(self.VARIABLES)))

        # Setup area of interest extraction
        boxsz = 0.1
        area_box = []
        area_box.append(self.args.latitude + boxsz)
        area_box.append(self.args.longitude - boxsz)
        area_box.append(self.args.latitude - boxsz)
        area_box.append(self.args.longitude + boxsz)

        if self.args.accum:
            times = []
            for i in range(24):
                times.append(time(hour=i, minute=0))
        else:
            times = [self.SAMPLE_TIME]

        self._download_era5_data(variables=self.VARIABLES,
                                 dates=self.args.dates,
                                 times=times,
                                 area=area_box,
                                 monthly=self.args.accum,
                                 out_file=self.download_file_path)

        if os.path.isfile(self.download_file_path):
            self.logger.info("ERA5 data was downloaded to '{}'.".format(self.download_file_path))
        else:
            raise FileNotFoundError("ERA5 download file '{}' was missing.".format(self.download_file_path))

        return self.download_file_path

    def _download_era5_data(self, variables: List[str], dates: List[date], times: List[time], area: str, monthly: str, out_file: str) -> None:
        """
        Executes the ERA5 download script in a separate process.
        :param variables: a list of variables to be downloaded from the Copernicus Climate Data Store.
        :param dates: a list of dates to download data for
        :param times: a list of times to download data for
        :param area: area of interest box to download data for
        :param monthly: download monthly reanalysis data
        :param out_file: output_file_path: path to the output file containing the requested fields.  Supported output format is NetCDF, determined by file extension.
        :return: nothing
        """

        if not os.path.exists(out_file):
            pexe = self.ERA_DOWNLOAD_PY.split(" ")
            if len(pexe) > 1:
                cmd = [pexe[0]]
                cmd.append(pexe[1])
            else:
                cmd = [self.ERA_DOWNLOAD_PY]
            cmd.extend(variables)
            cmd.extend(["--dates"] + [_date.strftime("%Y-%m-%d") for _date in dates])
            cmd.extend(["--times"] + [_time.strftime("%H:%M") for _time in times])
            cmd.extend(["--area", str(area)])
            if monthly:
                cmd.extend(["--monthly"])
            cmd.extend(["--out_file", out_file])
            self.logger.info("Download: {}".format(cmd))

            proc = subprocess.run(cmd)
            if not proc.returncode == 0:
                raise RuntimeError("Download process returned unexpected non-zero exit code '{}'.".format(proc.returncode))

        if not os.path.isfile(out_file):
            raise FileNotFoundError("Output file '{}' could not be located.".format(out_file))

class Era5DailyPrecipProcessing(Era5ProcessingBase):
    """
    Specialisation of the base class for downloading and processing precipitation data
    """
    #   variables to be downloaded using the API
    VARIABLES = ["total_precipitation"]

    @property
    def download_file_path(self):
        """
        Returns the path to the file that will be downloaded
        :return: path to the file that will be downloaded
        """
        return os.path.join(self.working_dir, "precip_{d}.nc".format(d=self.file_str))

    @property
    def output_file_path(self):
        """
        Returns the path to the output file from processing
        :return: path to the output file
        """
        return os.path.join(self.working_dir, "spi_{d}.json".format(d=self.file_str))

    def __init__(self, args, working_dir: str):
        """
        Initializer.  Forwards parameters to super class.
        :param args: program arguments
        :param working_dir: directory that will hold all files generated by the class
        """
        super().__init__(args=args, working_dir=working_dir)
        self.logger.debug("Initiated ERA5 daily processing of temperature strategy.")

    def download(self) -> str:
        """
        Performs file downloads.  Currently uses the basic functionality offered by the base class
        :return: path to the file containing the downloaded data
        """
        return super().download()

    def generate_json(self) -> None:
        """
         Generates OGC API Record JSON file
         :return: path to the json file
         """
        CONFIGURATION_FILE_PATH = os.path.join(os.path.dirname(__file__), "configuration-record.yaml")
        try:
            with open(CONFIGURATION_FILE_PATH, "r") as config_file:
                config = yaml.safe_load(config_file)
                catalog_id = config["catalog_id"]
                catalog_title = config["catalog_title"]
                catalog_desc = config["catalog_desc"]
                url = config["url"]
                temp = config["files"]
                files = temp.split(",")
                out_default = config["output_dir"]
                gsd = config["gsd"]
                yaml_file = config["yaml_file"]
                provider_name = config["provider_name"]
                provider_url = config["provider_url"]

                logging.debug("Configuration was loaded from '{}'.".format(CONFIGURATION_FILE_PATH))
        except (FileNotFoundError, IOError):
            logging.warning("Unable to load default configuration from '{}', relying on input variables.".format(
                CONFIGURATION_FILE_PATH))
            sys.exit(1)

        # Read version
        f = open(os.path.join(os.path.dirname(__file__), '__init__.py'), "r")
        version_file = f.read()
        version_line = re.search(r"^__version__ = ['\"]([^'\"]*)['\"]", version_file, re.M)
        self.logger.info("Running {}".format(version_line.group()))
        version = version_line.group().split("'")[1]

        # Extract date range
        dateval = datetime.utcnow()
        end_dateval = dateval
        print("Date range {} to {}".format(dateval, end_dateval))

        # Create catalog sub_folder - delete if exists
        cat_folder = os.path.join(outdir, "{}-record-v{}".format(catalog_id, version))
        if os.path.exists(cat_folder):
            shutil.rmtree(cat_folder)
        os.mkdir(cat_folder)

       # Create catalog information
        self.logger.info("Creating OGC Records Catalog")
        catalog_dict = {}
        catalog_dict.update({'cat_id': catalog_id})
        catalog_dict.update({'cat_description': catalog_desc})
        cat_begin = dateval.strftime("%Y-%m-%d")
        cat_end = end_dateval.strftime("%Y-%m-%d")
        catalog_dict.update({'cat_begin': cat_begin})
        catalog_dict.update({'cat_end': cat_end})

        # Loop for each file to create an OGC record for each
        link_dict = {}
        for count, file in enumerate(files):

            # For each file, update generic record yaml
            out_yaml = os.path.join(os.path.dirname(__file__), os.path.splitext(os.path.basename(yaml_file))[0] + "-updated.yml")

            # Read YML contents
            with open(os.path.join(os.path.dirname(__file__), yaml_file)) as f:
                # use safe_load instead load
                dataMap = yaml.safe_load(f)
                f.close()

            # Update bounding box
            self.logger.info("dataMap: {} ".format(dataMap['identification']['extents']['spatial']))
            yaml_dict = {}
            float_bbox = '[{:.3f},{:.3f},{:.3f},{:.3f}]'.format(bbox[0], bbox[1], bbox[2], bbox[3])
            yaml_dict.update({'bbox': ast.literal_eval(float_bbox)})
            yaml_dict.update({'crs': ast.literal_eval(dst_crs.split(":")[1])})
            # remove single quotes
            res = {key.replace("'", ""): val for key, val in yaml_dict.items()}
            dataMap['identification']['extents']['spatial'] = [res]
            self.logger.info("Modified dataMap: {} ".format(dataMap['identification']['extents']['spatial']))

            # Update dates
            self.logger.debug("dataMap: {} ".format(dataMap['identification']['extents']['temporal']))
            fdate = file.split("_")[0]
            dateval = datetime(int(fdate[0:4]), int(fdate[4:6]), int(fdate[6:8]), int(fdate[9:11]), int(fdate[11:13]),int(fdate[13:15]))
            date_string = dateval.strftime("%Y-%m-%d")
            end_date_string = end_dateval.strftime("%Y-%m-%d")

            yaml_dict = {}
            yaml_dict.update({'begin': date_string})
            yaml_dict.update({'end': end_date_string})
            dataMap['identification']['extents']['temporal'] = [yaml_dict]
            self.logger.debug("Modified dataMap: {} ".format(dataMap['identification']['extents']['temporal']))

            # Update filename
            self.logger.debug("dataMap: {} ".format(dataMap['metadata']['dataseturi']))
            dataMap['metadata']['dataseturi'] = url + file
            self.logger.debug("Modified dataMap: {} ".format(dataMap['metadata']['dataseturi']))

            # Updated url and file type
            dataMap['distribution']['s3']['url'] = url + file
            if os.path.splitext(file) == "tif":
                dataMap['distribution']['s3']['type'] = 'GeoTIFF'
            else:
                dataMap['distribution']['s3']['type'] = 'NetCDF'
            self.logger.debug("Modified dataMap type: {} ".format(dataMap['distribution']['s3']['type']))
            self.logger.debug("Modified dataMap url: {} ".format(dataMap['distribution']['s3']['url']))

            # Remove single quotes
            dataDict = {re.sub("'", "", key): val for key, val in dataMap.items()}

            # Output modified version of YAML
            with open(out_yaml, 'w') as f:
                yaml.dump(dataDict, f)
                f.close()

            # Read modified YAML into dictionary
            mcf_dict = read_mcf(out_yaml)

            # JSON dataset files
            dataset = "{}{}".format(os.path.basename(yaml_file).split(".")[0], count + 1)
            # create dataset folder
            dset_folder = os.path.join(cat_folder, dataset)
            os.mkdir(dset_folder)
            json_file = os.path.join(dset_folder, dataset + ".json")
            link_dict.update({dataset: "{}/".format(dataset) + os.path.basename(json_file)})

            # Choose API Records output schema
            records_os = OGCAPIRecordOutputSchema()

            # Default schema
            json_string = records_os.write(mcf_dict)

            # Write to disk
            with open(json_file, 'w') as ff:
                ff.write(json_string)
                ff.close()

            # Last loop
            if files[-1] == files[count]:

                # For the catalog, update generic record yaml
                cat_yaml = yaml_file.replace("record","catalog")
                out_yaml = os.path.join(os.path.dirname(__file__), os.path.splitext(os.path.basename(cat_yaml))[0] + "-updated.yml")

                # Read original YML contents
                print(out_yaml)
                with open(os.path.join(os.path.dirname(__file__), cat_yaml)) as f:
                    # use safe_load instead of load
                    dataMap = yaml.safe_load(f)
                    f.close()

                # Update details
                dataMap['identification']['extents']['spatial'] = [res]
                yaml_dict = {}
                yaml_dict.update({'begin': date_string})
                yaml_dict.update({'end': end_date_string})
                dataMap['identification']['extents']['temporal'] = [yaml_dict]

                # Remove single quotes
                dataDict = {re.sub("'", "", key): val for key, val in dataMap.items()}

                # Output modified version of YAML
                with open(out_yaml, 'w') as f:
                    yaml.dump(dataDict, f)
                    f.close()

                # Read modified YAML into dictionary
                mcf_dict = read_mcf(out_yaml)

                # Add record links
                catalog_dict.update({'cat_file': link_dict})
                mcf_dict.update(catalog_dict)

                # Catalog adjustments
                mcf_dict['metadata']['identifier'] = catalog_id
                mcf_dict['identification']['title'] = catalog_title
                mcf_dict['identification']['name'] = 'sam'
                mcf_dict['identification']['abstract'] = catalog_desc

                now_dateval = datetime.utcnow().strftime("%Y-%m-%d")

                mcf_dict['identification']['dates']['creation'] = now_dateval
                mcf_dict['identification']['dates']['revision'] = now_dateval
                mcf_dict['distribution']['s3']['url'] = link_dict
                print("Links: ",mcf_dict)
                # Choose API Dataset Record as catalog
                # https://github.com/cholmes/ogc-collection/blob/main/ogc-dataset-record-spec.md - see examples
                records_os = OGCAPIRecordOutputSchema()

                # Default catalog schema
                #print(mcf_dict)
                json_string = records_os.write(mcf_dict)
                logging.debug(json_string)

                # Write catalog to disk
                cat_file = os.path.join(cat_folder, "catalog.json")
                with open(cat_file, 'w') as ff:
                    ff.write(json_string)
                    ff.close()

        self.logger.info("Processing completed successfully for {}".format(cat_folder))


    def convert_precip_to_spi(self) -> None:
        """
        Calculates SPI precipitation drought index
        :param input_file_path: path to file containing precipitation
        :param output_file_path: path to file to be written containing SPI
        :return: nothing
        """

        # Extract data from NetCDF file
        datxr = xarray.open_dataset(self.download_file_path)
        self.logger.debug("Xarray:")
        self.logger.debug(datxr)

        # Convert to monthly sums and extract max of the available cells
        # group('time.month') is 1 to 12 while resamp is monthly data
        if self.args.accum:
            resamp = datxr.tp.max(['latitude', 'longitude']).load()
        else:
            resamp = datxr.tp.resample(time='1MS').sum().max(['latitude', 'longitude']).load()
        precip = resamp[:, 0]

        self.logger.info("Input precipitation, {} values: {:.3f} {:.3f} ".format(len(precip.values), np.nanmin(precip.values), np.nanmax(precip.values)))

        # Calculate SPI
        spi = indices.INDICES(self.args)
        spi_vals = spi.calc_spi(np.array(precip.values).flatten())
        self.logger.info("SPI, {} values: {:.3f} {:.3f}".format(len(spi_vals), np.nanmin(spi_vals),np.nanmax(spi_vals)))
        resamp = resamp.sel(expver=1, drop=True)

        # Convert xarray to dataframe Series and add SPI
        df = resamp.to_dataframe()
        df['spi'] = spi_vals
        #df = df.reset_index(level=[1,2])
        self.logger.debug("DF: ")
        self.logger.debug(df.head())

        # Select requested time slice
        sdate = r'{}-{}-{}'.format(self.args.start_date[0:4],self.args.start_date[4:6],self.args.start_date[6:8])
        edate = r'{}-{}-{}'.format(self.args.end_date[0:4],self.args.end_date[4:6],self.args.end_date[6:8])
        self.logger.debug("Filtering between {} and {}".format(sdate, edate))
        df_filtered = df.loc[(df.index >= sdate) & (df.index <= edate)]

        # Convert date/time to string and then set this as the index
        df_filtered['day'] = df_filtered.index.strftime('%Y-%m')#-%d')
        df_filtered = df_filtered.reset_index(drop=True)
        df_filtered = df_filtered.set_index('day')
        #df_filtered = df_filtered.drop(['latitude'], axis=1)
        #df_filtered = df_filtered.drop(['longitude'], axis=1)
        self.logger.debug("Updated DF: ")
        self.logger.debug(df_filtered.head())

        # Scatter plot
        if self.args.plot:
            fig = plt.figure(dpi=900)
            ax1 = df_filtered['tp'].plot(label='Total precip')
            ax2 = df_filtered['spi'].plot(secondary_y = True, label='SPI')
            ax1.set_xlabel('Time')
            ax1.set_ylabel('Total Precipitation [m, blue]')
            ax2.set_ylabel('Standardized Precipitation Index (SPI, orange)')
            ax1.grid(True, linestyle = ':')
            ax2.grid(True, linestyle=':')
            pngfile = os.path.join(os.path.dirname(self.output_file_path),"{}-plot.png".format(self.args.product))
            self.logger.debug("PNG: {}".format(pngfile))
            plt.savefig(pngfile)

        # Save as json file
        json_str = df_filtered.to_json()
        self.logger.debug("JSON: {}".format(json_str))
        with open(self.output_file_path, "w") as outfile:
            json.dump(json_str, outfile, indent=4)

    def process(self) -> str:
        """
        Carries out processing of the downloaded data.  This is the main functionality that is likely to differ between
        each implementation.
        :return: path to the output file generated by the algorithm
        """
        self.logger.info("Initiating processing of ERA5 daily data.")

        if not os.path.isfile(self.download_file_path):
            raise FileNotFoundError("Unable to locate downloaded data '{}'.".format(self.download_file_path))

        # Calculates SPI precipitation drought index
        self.convert_precip_to_spi()

        return self.output_file_path






